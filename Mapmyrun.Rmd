---
title: "MapMyrun"
output: html_document
---

#Introduction

I want a map of Baltimore that shows, for each street, how popular it is as a 
running location. I will use the nearly half million runs posted for Baltimore 
city and county over the last ten years on MapMyRun.com. To access these runs, 
we need to download each as a .gpx file, which would be inconvenient, to say 
the least, to do by hand. Thus, we will use the Selenium WebDriver to automate
the downloading of these files. 

To access the runs posted for Baltimore, let's start at 
[MapMyRun's site](http://mapmyrun.com/routes). In `Near`, we type 
"Baltimore, MD, USA", we select `At Least`  0 miles, and put in `City`.

![](C:/Users/Jesse/Programming/R/Map My Run Redone/Images/search parameters initial.jpg)

*** 

What does this search return?

***

![](table initial.jpg)

***

As we can see, at the time of data collection, there were 359,394 routes posted.
The table above displays the 19 most recent runs. Importantly, not only is 
MapMyRun widely used but it is up to date. When I accessed the site on September 
1st at 2:00PM, 24 runs had already been posted that day. Furthermore, these runs 
are not confined to the city and clearly extend into the county, which will give 
our map a wider range. 

Let's visualize the runs geographically.

***

![](initial map not satellite.jpg)

***

Just for fun, let's see what this looks like in satellite view. (This image was 
accessed later, which explains why the runs changed)

***

![](initial_search_terrain_view.jpg)

***

#Accessing the posted routes

There are nearly 400,000 posted routes on MapMyRun in and around Baltimore city.
But how can we turn routes into tidy tables? We need to use GPS coordinates. 

GPS stands for Global Positioning System and consists of 27 satellites in medium 
Earth orbit owned by the United States government and operated by the Air Force. 
Interestingly, since the US owns them, the US can selectively deny access to the 
GPS system, which it did to the Indian government in 1999 during the Kargil War. 
(An aside: The Kargil War, fought between India and Pakistan, is actually super 
interesting. The war, during which two Pakistani officials threatened to use 
nuclear weapons, may have been masterminded in secret by Pakistani General Perez 
Musharraf, who subsequently deposed the prime minister Nawaz Sharif in a 
military coup when Sharif attempted to court martial him after Pakistan lost 
spectactularly.) This "right to refuse service" prompted India to create their 
own GPS system, called NAVIC. A number of non-American GPS systems now exist, 
including the EU's Galileo and Russia's GLONASS (interesting name).

The 27 GPS satellites (24 in use and 3 as back-up in case others fail) are 
positioned such that any user anywhere on Earth can access four satellites, 
which is the necessary number for accurate positioning. The GPS receiver in your 
phone determines the distance to each satellite and combines that information to 
deduce the receiver's own location through 
[trilaterion](https://electronics.howstuffworks.com/gadgets/travel/gps.htm). 
Each GPS satellite is 
[constantly transmitting radio waves](http://www.physics.org/article-questions.asp?id=55) 
which contain the 
time and location at which they were sent. When these signals reach your 
receiver, the difference in time between when they were sent and received 
multiplied by the speed of light is the distance between you and the satellite. 
Knowing your distance from a point places you *somewhere* on a sphere 
surrounding it. Your location is thus the intersection of the three spheres 
centered on each satellite, which is trilateration. In fact, only three 
satellites are necessary to identify your location and the fourth is for 
increased accuracy.

![Shown using circles instead of spheres for simplicity](trilateration.jpg)

As our MapMyRun user sweats through his workout, his receiver constantly uses 
this strategy to determine his location, sometimes with an accuracy measured in 
inches! Of the vast number of GPS coordinates in Baltimore, his specific 
latitude and longitude is constantly being identified. In essence, a route can 
be characterized as a set of binary Yes and No's. Just as a river is defined by 
where it doesn't run as much as where it does, a route is informative not only 
because of where it does go but where it doesn't. We will pinpoint the GPS 
"hotspots" of the city.

#Download the information from MapMyRun

For each run posted on MapMyRun, we want to download the route as a list of GPS 
coordinates (i.e., latitudes and longitudes) denoting where the runner was 
throughout. 

This data is contained for each run as a `.gpx` file. GPX, which stands for GPS 
EXchange format, is a specific type of markup language, written in an XML 
schema. That begs the question, what exactly is a markup language?

From [wikipedia](https://en.wikipedia.org/wiki/Markup_language):

"Markup (languages) instruct the software that displays the text to carry out 
appropriate actions, but are omitted from the version of the text that users 
see."

Simply, markup is code written around content which determines how that content 
will be formatted. The most famous markup language is HTML. HTML determines how 
content on a website is displayed, transforming it from what would be a .txt 
file to the interactive websites we use today. The components of a "markup 
language" describe the identity and function of each part of the document, 
saying, for instance "this is the heading", etc.

I have downloaded one of these very .gpx files, so let's open it, see how it 
looks, and plot it. 

#Libraries for project

```{r, message=FALSE}

#libraries
library(tidyverse)
library(RSelenium)
library(data.table) #to read in big excel files
library(beepr)
library(leaflet)
memory.limit(size = 32000000000)

#library(XML)
library(rgdal) #this is used for readOGR
library(ggmap)
 #library(devtools)
```

#Plotting a .gpx file

The function `readOGR` from the `rgdal` package turns it into a S4 Object of 
class Spacial Lines Data Frame.  

```{r}
#Using the readOGR function from rgdal package.
first_try <- readOGR(dsn = "route842022275.gpx", "tracks")

class(first_try)
```

This object can be plotted using `plot`. (Soon we will get into some much more 
interactive mapping techniques but first let's see the basics.)

```{r}
plot(first_try)
```

(put it against a real backdrop here)- do some real mapping. (either that or 
don't show the mapping at all)

This appears to be a run, possibly around some body of water, but it's 
impossible to say without seeing it against the backdrop of Baltimore. 

We use the `coordinates` function to turn our S4 Object into a normal data 
frame.

```{r}
first_coord <- coordinates(first_try)[[1]][[1]] %>%
  as.tibble()

#set colnames
colnames(first_coord) <- c("long", "lat")

head(first_coord)
```

#Scraping .gpx files for all runs
##Setting up Selenium quickly

We can obtain a .gpx file from MapMyRun's website and turn it into a tidy tibble 
containing all coordinates ran. Now the question is can we do this for all of 
the posted runs? We can collect the data by either web scraping or using 
MapMyRun's API. Unfortunately, MapMyRun never returned my request for an API 
key- so web scraping is our only option.

I published a tutorial on everything you need to set up Docker and Selenium


```{r}

#This setting is very important. You need to set your default directory to a 
#directory in Docker, that you then connect to your own computer. 
eCaps <- list(
  chromeOptions = 
    list(prefs = list(
      "profile.default_content_settings.popups" = 0L,
      "download.prompt_for_download" = FALSE,
      "download.default_directory" = "home/seluser/Downloads"
    )
    )
)

remDr <- remoteDriver(remoteServerAddr = "192.168.99.100", browserName= "chrome"
                      , port=4445L, extraCapabilities = eCaps)

remDr$open()
```

To download routes in MapMyRun, it is necessary to first login to your account. 
So we start by accessing the MapMyRun homepage. 

##How to Do it for One .gpx run
```{r}
#go to mapmyrun home page
remDr$navigate("https://www.mapmyrun.com")

remDr$screenshot(display = TRUE)

#it seems first i must sign to the website

#go to login thing
login <- remDr$findElement(using = "xpath", "//header/div[2]/nav[2]/ul/li[1]/a")
login$clickElement()
remDr$screenshot(display = TRUE)

#Now we need to login

#input email
login <- remDr$findElement(using = 'name', value = "email")
login$sendKeysToElement(list("kerr.jesse@gmail.com"))
remDr$screenshot(display = TRUE)

#input password
login <- remDr$findElement(using = 'name', value = "password")
login$sendKeysToElement(list("NewJustforMap"))
remDr$screenshot(display = TRUE)

#click LogIn
login <- remDr$findElement(using = 'css', "button")
login$clickElement()
remDr$screenshot(display = TRUE)
```


```{r}

#we're in! Now I want to go to the routes page. It's in a dropdown, so I need to 
#navigate there to open the dropdown. 

routes <- remDr$findElement(using = 
                              "xpath", "//header/div[1]/nav[1]/ul/li[2]/a")
remDr$mouseMoveToLocation(webElement = routes)
remDr$screenshot(display = TRUE)

#now the dropdown is open so let's click on the first one.
routes <- remDr$findElement(using = "xpath", 
                            "//header/div[1]/nav[1]/ul/li[2]/ul/li/a")

#Using `getElementAttribute` we can verify whether we have the right link.
routes$getElementAttribute("text")

#It returns "Find Routes"!!!!
routes$clickElement()
remDr$getCurrentUrl()
remDr$screenshot(display = TRUE)
```

`remDr$getTitle` tells you what the site has been named. 
`remDr$getCurrentUrl()` tells you the URL you are at. 

Now we are at the website--- what are we doing? We need to tell Selenium to put 
information somewhere, download the URLs that come up, then go to the next page. 

```{r}
#the website now has "United States" in the "Near" selection. Need to select 
#that, then delete that.
webElem <- remDr$findElement(using = "class", value="Select-clear")

#removes the United States
webElem$clickElement()

remDr$screenshot(display = TRUE)

#we need the input element
webElem <- remDr$findElements(using = "css", value="input")

#there are 4 of these, the second is the input we want. 
city <- webElem[[2]]

#we can see, in accordance with the page source, that there is no class, id, or 
#name associated with this element. 

city$getElementAttribute("class")

city$getElementAttribute("id")

city$getElementAttribute("name")

#below is the code for "tab", to fill in the Baltimore, MD, USA thing.
#for some reason sometimes this requires being done twice.
city$sendKeysToElement(list("Baltimore, MD, USA", "\uE004"))
remDr$screenshot(display = TRUE)

#Now we need to click the search button
webElem <- remDr$findElement("xpath", 
                             "//div[@class = 
                             'submitButtonDesktop-36_Oa']/button[@class = 
                             'primary-xvWQU button-2M08K medium-3PyzS']")

webElem$clickElement()
remDr$screenshot(display = TRUE)

#want to scroll to the bottom
webElem <- remDr$findElement("css", "body")
webElem$sendKeysToElement(list(key = "end"))
remDr$screenshot(display = TRUE)

```

We have finally managed to load the site. Now I want to find all of the elements 
that are my links. The parts below needs to be repeated.

```{r}
#this gives 20, which is correct
links <- remDr$findElements('xpath', "//a[contains(@href, 'routes/view')]")

#write a for loop to save these?
#initialize a dataframe/ tibble
runs <- as.tibble(matrix(nrow = 20))

for (i in 1:length(links)){
  yo <- links[[i]]
  new <- yo$getElementAttribute("href")
  runs[i,1] <- paste(new, sep="")
}

```

How do we click next to go to the next one?

```{r}
webElem <- remDr$findElement(using = 'xpath', 
                             "//*[@id='root']/div/div[3]/div/div[4]/a")
webElem <- remDr$findElement(using = 'xpath', 
                             "//a[@class= 'pageLink-3961h']/span")
length(webElem)
webElem$clickElement()
remDr$screenshot(display = TRUE)
```

Now, can I loop through all of the webpages, saving them all in the table?

There are now 383,321 uploaded runs. If there are 20 per page, we need to access 
19,191 pages

#The repeated Process - after being logged in.
```{r}
#whole process

#first page
remDr$navigate("https://mapmyrun.com/routes")
remDr$screenshot(display = TRUE)

#scroll down to make sure the web element is on the page. 
webElem2 <- remDr$findElement("css", "body")
webElem2$sendKeysToElement(list(key = "start"))
remDr$screenshot(display = TRUE)

#change window size to make sure web element is there. 
remDr$setWindowSize(1096, 1604)
remDr$screenshot(display = TRUE)

#initialize starter matrix
runs <- as.tibble(matrix(nrow = 20))

#this first part is just for the first page. 
#save the links on this first page
links <- remDr$findElements('xpath', "//a[contains(@href, 'routes/view')]")

#write a for loop to save these 20 links on this first page as a tibble. 
for (i in 1:length(links)){
  yo <- links[[i]]
  new <- yo$getElementAttribute("href")
  runs[i,1] <- paste(new, sep="")
}

runs <- rbind(as.tibble(matrix(nrow = 20)), runs)

#go to next
webElem <- remDr$findElement(using = 'xpath', 
                             "//a[@class= 'pageLink-3961h']/span")
webElem$clickElement()
remDr$screenshot(display = TRUE)

#Now here is the while loop that will truly repeat. 
#this tells it, as long as at the bottom of the page there is a previous and 
#next button, to go ahead and download the links specified above on the page. 
while (length(remDr$findElements(using = 'xpath', 
                                 "//a[@class= 'pageLink-3961h']/span")) == 2){
  links <- remDr$findElements('xpath', "//a[contains(@href, 'routes/view')]")

for (i in 1:length(links)){
  yo <- links[[i]]
  new <- yo$getElementAttribute("href")
  runs[i,1] <- paste(new, sep="")
}
  runs <- rbind(as.tibble(matrix(nrow = 20)), runs)
  webElem <- remDr$findElement(using = 'xpath', 
                               "//a[@class= 'pageLink-3961h'][2]/span")
  webElem$clickElement()
  Sys.sleep(10) 
  remDr$screenshot(display = TRUE)
}

#We wrote all of this to the CSV entitled Runs. 
#There are 56060 URLs rows, except 20 are NAs. However, some of them are
#repeated because I had to rerun the loop and did not cut off exactly correctly.
#Therefore, after reading in the csv, I am going to see how many unique 
#values there are. 
URLS <- read_csv("runs.csv")
URLS <- unique(URLS)

#So there was an overlap of 6,000 or so URLs, and our final number is 44,882. 
#I am now going to save this as "URL_links_to_gpx_files.csv" and delete runs.csv

write_csv(URLS, "URL_links_to_gpx_files.csv")

```

With my list of URLS, I am now prepared to begin downloading .gpx files. 

It shouldn't be too hard to tell Selenium to go to that page and download that 
specific file. 

This step is going to each URL, clicking on download .gpx, then going to the
next URL and repeating. So in this step I did not need to save anything, it was
only putting files in a folder. 

```{r}
#give Selenium a URL and tell it to navigate to it. 
remDr$navigate("https://www.mapmyrun.com/routes/view/516461260")
remDr$screenshot(display = TRUE)

webeL <- remDr$findElement(using = "xpath", "//a[@id = 'export_this_route']")
webeL$clickElement()
remDr$screenshot(display = TRUE)

webEl <- remDr$findElement(using = "xpath", 
                           "//input[@id = 'export_route_gpx_btn']")
webEl$clickElement()
remDr$screenshot(display = TRUE)

```


It worked for one. Now it needs to go for the whole loop.
#Loop to download all

```{r}

#sets the amount of time in milliseconds that the driver should wait when 
#searching for elements. Essential!
remDr$setTimeout(type = "implicit", milliseconds = 100000)

#sets how long it should wait for the page to load
remDr$setTimeout(type = "page load", milliseconds = 100000)

for (i in 1:20000){

  #Go through the URL from the URL_links_to_gpx_files.csv
  remDr$navigate(paste0(URLS[i,]))
  webeL1 <- remDr$findElement(using = "xpath", "//a[@id = 'export_this_route']")
  
#often Selenium would click to open the widget, but the widget wouldn't open. 
#So I set a while loop- as long as the html has not changed, keep on clicking. 
  while(length(remDr$findElements(using= "xpath", 
                                  "//div[@class='ui-widget-overlay']"))==0){
        webeL1$clickElement()
    }
# We need to find the change that occurs on the page when the export 
# button is clicked; that is the only way. 
# Coupled with this solution is the idea that sometimes it clicks but doesn't go 
# to the next page, then the best idea would be simply to ask it to click once 
# again I believe. For that I use a while loop that asks whether the expected 
# product has appeared, until it has appeared, continue clicking. 
# the part of the html that changes when that button is clicked. This tells 
# Selenium to wait until this is findable to try to click the next button.
  webEl2 <- remDr$findElement(
    using = "xpath", 
    "//form[@id= 'export_route_gpx_form']/p[2]/input[@name = 'btnExportGPX' and 
    @value='Download GPX File']")
  webEl2$clickElement()
}

#figure out where you ended up at. 
newRLS <- URLS %>%
  mutate(row = 1:nrow(URLS)) %>%
  filter(str_detect(V1, "628079342"))
```


#Combine the .gpx's
```{r}
#first, get the names of all the gps's we have in the folder. We use the regex
#"gpx$" to say that it ends with gpx
List_of_all_downloaded_gpx_files <- list.files(
path = "C:/Users/Jesse/Programming/R/Map My Run Redone/.gpx files", 
                   pattern = "gpx$") %>%
  as.tibble()

#I was able to verify that there is no duplication. There are exactly 18298 .gpx
#files. 

#We save this information
write_csv(List_of_all_downloaded_gpx_files, 
          "List_of_all_downloaded_gpx_files.csv")

#Now we want to open all of those and turn them into coordinates. 
#Use the readOGR function to get their coords.
#Here is the example
#these steps create the coordinates.
gps <- readOGR(dsn = paste0(List_of_all_downloaded_gpx_files[1,]), "tracks")
gps <- coordinates(gps)[[1]][[1]]%>%
    as.tibble() %>%
    unique()

#this step labels it with its URL, from which we could later obtain dates, 
#even the name of the person who did the run. 
gps <- gps %>%
  mutate(RunID = paste0(List_of_all_downloaded_gpx_files[1,]))

#Now we do this as a loop. 
for (i in 1:nrow(List_of_all_downloaded_gpx_files)){

#create a file called gps that each coordinate will be added on to.
#create a temporary file called gps2 for each coordinate. 
  gps2 <- readOGR(dsn = paste0(List_of_all_downloaded_gpx_files[i,]), "tracks") 
  gps2 <- coordinates(gps2)[[1]][[1]] %>%
    as.tibble() %>%
    unique()
  
  gps2 <- gps2 %>%
    mutate(RunID = paste0(List_of_all_downloaded_gpx_files[i,]))
    
  gps <- rbind(gps, gps2)
}

#This was done in 5 iterations. In the end these were bound together to form
#allCoordinates.csv

#let's read that in. 
all_coordinates <- fread(
  "all_coordinates_possibly_rounded_but_probably_unfiltered.csv")

#I need to check whether I turned all of the .gpx files into coordinates. 
list_of_gpx_files_convered_to_coordinates <- all_coordinates %>%
  group_by(RunID) %>%
  summarise(n=n())

#it appears that there 18293 gpx files that were converted. So I am missing 5, 
#luckily, that's nothing to lose sleep over.

#save this list!
write_csv(list_of_gpx_files_convered_to_coordinates, 
          "List_of_gpx_files_convered_to_coordinates.csv")

```

#Calculating the number of each coordinate
#Normalizing the data but not by rounding, by best 5 for each 4. 

For the lat and lon coords, they are all currently 7 sig figs. 
Or at least 6, at any rate, 5 after the decimal. 

We want to normalize these, because in those last 2 sig figs there is a great 
deal of individual variation (i.e., if the person ran a foot north of the other 
on the same street.)

However, we can't simply round, because then we're getting, for example, 
76.43200, and that 00 at the end may not represent an actual point people run, 
it could be a home. 

So, for each 5 sig fig combo of lat and lon, e.g., 76.4343 and -32.3454,
using groupby, we'll say return the 7 decimal one, with these 5 decimal 
starting, that occurs more than others. Then change all the other 7 decimals 
with those 5 decimals starting to the most common 7 decimal for those 5 decimals. 

If there are two with the same number? I'm thinking random? Unsure. 

That means that you will still need to filter, so that the same coordinate is 
not double counted for one runner, in case two of his coordinates end up the 
same.

We could try first with "rounding" to six sig figs, then try with 5. 

```{r}
#Here, I'm going to redo this analysis (the counting) with the unrounded data. 
unrounded_all_coordinates <- read_csv(
  "./Data/Working with the coord data/all_coordinates_not_rounded_or_unfiltered 1.csv")

#we will round to 5 digits after decimal to simplify the analysis. 
unrounded_all_coordinates <- unrounded_all_coordinates %>%
  mutate_at(.vars = c("V1", "V2"), funs(round(., 5)))

#now since we rounded we must do the unique, in case any doubled up on one run. 
unrounded_all_coordinates <- unique(unrounded_all_coordinates)

write_csv(
  unrounded_all_coordinates, 
  "./Data/Working with the coord data/all_coordinates_rounded_5 digs_and_filtered.csv")

unrounded_all_coordinates <- read_csv(
  "./Data/Working with the coord data/all_coordinates_rounded_5 digs_and_filtered.csv")

#This has 30 million lines. 
#can't simply summarise because we need to hold on to the .gpx file URL to later
#filter out duplicated. 
count_all_unrounded_coordinates <- unrounded_all_coordinates %>%
  group_by(V1, V2) %>%
  mutate(n=n())

#must ungroup after
count_all_unrounded_coordinates <- count_all_unrounded_coordinates %>%
  ungroup()

colnames(count_all_unrounded_coordinates) <- c("lon", "lat", "RunID", "n")

count_all_unrounded_coordinates <- count_all_unrounded_coordinates %>%
  select(lat, lon, n, RunID)

#write_csv(count_all_unrounded_coordinates,
          "./Data/Working with the coord data/rounded_5_digs_filtered_then_counted_with_run_ID.txt")

#count_all_unrounded_coordinates <- fread(
  "./Data/Working with the coord data/rounded_5_digs_filtered_then_counted_with_run_ID.txt")

#split into whole numbers and decimals, then into first 4 after decimal then
#four after that
count_all_unrounded_coordinates <- count_all_unrounded_coordinates %>%
  separate(col = lon, sep = "\\.", 
           into = c("lon_whole_nums", "lon_after_decimal"), convert = TRUE) %>%
  separate(col = lon_after_decimal, sep = 4, 
           into = c("lon_first_4_after_decimal", 
                    "lon_more_than_4_after_decimal"), convert = TRUE) %>%
  separate(col = lat, sep = "\\.", 
           into = c("lat_whole_nums", "lat_after_decimal"), convert = TRUE) %>%
  separate(col = lat_after_decimal, sep = 4, 
           into = c("lat_first_4_after_decimal", 
                    "lat_more_than_4_after_decimal"), convert = TRUE) %>%
#all the zeros become NA;s, which we need to fix
  mutate_all(funs(replace_na(.,0)))

#Now unite the whole numbers and the 4 after the decimal for lon and lat. 
count_all_unrounded_coordinates <- count_all_unrounded_coordinates %>%
  unite("lon_first_4", c(lon_whole_nums, lon_first_4_after_decimal), 
        sep = '.') %>%
  unite("lat_first_4", c(lat_whole_nums, lat_first_4_after_decimal), 
        sep = '.')

#now I need to group by lat_first_4 and lon_first_4, and ask for each 
#then pick the one with the highest n.

#It appears that which.max, if there is the same value repeated,  
count_all_unrounded_coordinates <- count_all_unrounded_coordinates %>%
  group_by(lon_first_4, lat_first_4) %>%
  mutate(lon_more_than_4_after_decimal = lon_more_than_4_after_decimal[which.max(n)]) %>%
  mutate(lat_more_than_4_after_decimal = lat_more_than_4_after_decimal[which.max(n)])

#now, we unite them back together. 
count_all_unrounded_coordinates <- count_all_unrounded_coordinates %>%
  unite("lon", c(lon_first_4, lon_more_than_4_after_decimal), 
        sep = '') %>%
  unite("lat", c(lat_first_4, lat_more_than_4_after_decimal), 
        sep = '')

#save at this point.
write_csv(count_all_unrounded_coordinates,
          "./Data/Working with the coord data/rounded_5_digs_filtered_counted_then_normalized_to_max_but_not_yet_filtered.txt")

#now that I have changed values, I need to do the unique thing again, in case
#2 values for one run got the same value. ALso let's remove n first. 
count_all_unrounded_coordinates <- count_all_unrounded_coordinates %>%
  select(-n)

count_all_unrounded_coordinates <- unique(count_all_unrounded_coordinates)
  
#save at this point.
write_csv(count_all_unrounded_coordinates,
          "./Data/Working with the coord data/rounded_5_digs_filtered_counted_then_normalized_to_max_and_filtered.txt")

count_all_unrounded_coordinates <- count_all_unrounded_coordinates %>%
  select(-RunID) %>%
  group_by(lat, lon) %>%
  summarise(n=n()) %>%
  ungroup()

#save at this point.
write_csv(count_all_unrounded_coordinates,
          "./Data/Working with the coord data/rounded_5_digs_filtered_counted_then_normalized_to_best_5_dig_and_filtered_then_counted_again.csv")

count_all_unrounded_coordinates <- read_csv("./Data/Working with the coord data/5 digs NORMALIZATION process/rounded_5_digs_filtered_counted_then_normalized_to_best_5_dig_and_filtered_then_counted_again.csv")

#remove the ones outside of the city or county
#39.2, -76.8 is near Columbia, which seems fair, and 39.7, -76.3 is nearly in 
#Lancaster. I also had some reason for doing this but now can't remember, but 
#these numbers seem fine.  
count_all_unrounded_coordinates <- count_all_unrounded_coordinates %>%
  filter(lon > -76.8 & lon < -76.3 & lat > 39.2 & lat < 39.7)

#save at this point.
write_csv(count_all_unrounded_coordinates,
          "./Data/Working with the coord data/rounded_5_digs_filtered_counted_then_normalized_to_best_5_dig_and_filtered_then_counted_again_then_filtered_for_Bmore_only")

#get the top counts:
count_all_unrounded_coordinates %>%
  arrange(desc(n)) %>%
  head(50)
#top is 592 bitch.

```

Looking at the map it is clear that, for three after the decimal, I should 
choose the best fourth and fifth. 

Or can I just round to 4 and then choose the best 4th? It seems to make sense, 
if only because I need higher n's, to make sure the coordinate that I am 
changing the other ones to is more accurate. 

Because if I leave it at 5, when I count, most will be 1's, and the three 
that gets chosen is very arbitrary. I want to use actual numbers of times 
people run the area to choose which 3 to pick, which is more likely to be 
informative if rounded to the fourth. 

```{r}
#Here, I'm going to redo this analysis (the counting) with the unrounded data. 
unrounded_all_coordinates <- fread(
  "./Data/Working with the coord data/all_coordinates_not_rounded_or_unfiltered 1.csv")

#we will round to 4 digits after decimal to get a higher n, to help the algorithm
#choose which point to make all points.
unrounded_all_coordinates <- unrounded_all_coordinates %>%
  mutate_at(.vars = c("V1", "V2"), funs(round(., 4)))

#it's at 30,308,840 rows. It really should decrease after this rounding, right?
#now since we rounded we must do the unique, in case any doubled up on one run. 
unrounded_all_coordinates <- unique(unrounded_all_coordinates)
#it doesn't decrease, which means that the .gpx file, originally, does not
#take more than 1 per 4th decimal point. 

#This has 30 million lines. 
#can't simply summarise because we need to hold on to the .gpx file URL to later
#filter out duplicated. 
unrounded_all_coordinates <- unrounded_all_coordinates %>%
  group_by(V1, V2) %>%
  mutate(n=n())

#must ungroup after
unrounded_all_coordinates <- unrounded_all_coordinates %>%
  ungroup()

colnames(unrounded_all_coordinates) <- c("lon", "lat", "RunID", "n")

unrounded_all_coordinates <- unrounded_all_coordinates %>%
  select(lat, lon, n, RunID)

write_csv(unrounded_all_coordinates,
          "./Data/Working with the coord data/3 digs rounding process/rounded_4_digs_filtered_then_counted_with_run_ID.csv")

unrounded_all_coordinates <- fread(
  "./Data/Working with the coord data/3 digs rounding process/rounded_4_digs_filtered_then_counted_with_run_ID.csv")

#split into whole numbers and decimals, then into first 4 after decimal then
#four after that
unrounded_all_coordinates <- unrounded_all_coordinates %>%
  separate(col = lon, sep = "\\.", 
           into = c("lon_whole_nums", "lon_after_decimal"), convert = TRUE) %>%
  separate(col = lon_after_decimal, sep = 3, 
           into = c("lon_first_4_after_decimal", 
                    "lon_more_than_4_after_decimal"), convert = TRUE) %>%
  separate(col = lat, sep = "\\.", 
           into = c("lat_whole_nums", "lat_after_decimal"), convert = TRUE) %>%
  separate(col = lat_after_decimal, sep = 3, 
           into = c("lat_first_4_after_decimal", 
                    "lat_more_than_4_after_decimal"), convert = TRUE) %>%
#all the zeros become NA;s, which we need to fix
  mutate_all(funs(replace_na(.,0)))

#Now unite the whole numbers and the 4 after the decimal for lon and lat. 
unrounded_all_coordinates <- unrounded_all_coordinates %>%
  unite("lon_first_4", c(lon_whole_nums, lon_first_4_after_decimal), 
        sep = '.') %>%
  unite("lat_first_4", c(lat_whole_nums, lat_first_4_after_decimal), 
        sep = '.')

unrounded_all_coordinates <- unrounded_all_coordinates %>%
  mutate(lon_first_4 = as.numeric(lon_first_4, digits = 6),
         lat_first_4 = as.numeric(lat_first_4, digits = 7))

#now I need to group by lat_first_4 and lon_first_4, and ask for each 
#then pick the one with the highest n.

#It appears that which.max, if there is the same value repeated,  
unrounded_all_coordinates <- unrounded_all_coordinates %>%
  group_by(lon_first_4, lat_first_4) %>%
  mutate(lon_more_than_4_after_decimal = lon_more_than_4_after_decimal[which.max(n)]) %>%
  mutate(lat_more_than_4_after_decimal = lat_more_than_4_after_decimal[which.max(n)])

#now, we unite them back together. 
unrounded_all_coordinates <- unrounded_all_coordinates %>%
  unite("lon", c(lon_first_4, lon_more_than_4_after_decimal), 
        sep = '') %>%
  unite("lat", c(lat_first_4, lat_more_than_4_after_decimal), 
        sep = '')

unrounded_all_coordinates <- unrounded_all_coordinates %>%
  mutate(lon = as.numeric(lon, digits = 6),
         lat = as.numeric(lat, digits = 6))

#now that I have changed values, I need to do the unique thing again, in case
#2 values for one run got the same value. ALso let's remove n first. 
unrounded_all_coordinates <- unrounded_all_coordinates %>%
  select(-n)

unrounded_all_coordinates <- unique(unrounded_all_coordinates)

#save!
write_csv(unrounded_all_coordinates,
"best_jesse.csv")

unrounded_all_coordinates <- unrounded_all_coordinates %>%
  group_by(lat, lon) %>%
  summarise(n=n()) %>%
  ungroup()

#remove the ones outside of the city or county
#39.2, -76.8 is near Columbia, which seems fair, and 39.7, -76.3 is nearly in 
#Lancaster. I also had some reason for doing this but now can't remember, but 
#these numbers seem fine.  
unrounded_all_coordinates <- unrounded_all_coordinates %>%
  filter(lon > -76.8 & lon < -76.3 & lat > 39.2 & lat < 39.7)

write_csv(unrounded_all_coordinates,
          "./Data/Working with the coord data/3 digs rounding process/3_digs_filtered_redone_as_numeric_final.csv")
  
#So this new analysis has made sure that things were numeric at every step-
#very important. 

prac <- unrounded_all_coordinates[1:500,]

prac2 <- prac %>%
  mutate(lat2 = pmap_chr(list(x = lat, y = lon, v = n),
                                 function(x, y, v)
                                 ifelse(any(lat_Lower_Range < x & 
                                              lat_Upper_Range > x & 
                                              lon_Lower_Range < y & 
                                              lon_Upper_Range > y & 
                                              v > n) == TRUE,
                                 paste(lat[which(lat_Lower_Range < x & 
                                              lat_Upper_Range > x & 
                                              lon_Lower_Range < y & 
                                              lon_Upper_Range > y & 
                                              v > n)], collapse = ", "),
                                 lat)))
unrounded_all_coordinates <- unrounded_all_coordinates %>%
  mutate(lat_Lower_Range = lat - 0.0001,
         lat_Upper_Range = lat + 0.0001,
         lon_Lower_Range = lon - 0.0001,
         lon_Upper_Range = lon + 0.0001)

#get the top counts:
unrounded_all_coordinates %>%
  arrange(desc(n)) %>%
  head(50)
#top is 957 bitch.
```

The problem that I have now ran into is that, at some point of my analysis, 
the precision after the decimal is changing, i.e., from 32 to 32.000, and that
is messing me up in all sorts of ways. I want to learn a couple things: 
a)Could it have to do with difference between read_csv and fread
b) Could there be difference in data.table and data.frame
c) how the fuck do i fix it. 

It seems to me that the best way to do it is to say: If we round to 3 after 
decimal and we are the same, then take the one that happens more. 
#Leaflet section


##Log transform the data

```{r}
count_all_coords <- read_csv(
  "./Data/Working with the coord data/3 digs rounding process/3_digs_filtered_redone_as_numeric_final.csv")

#Look at both Log2 and log10. Log 10 works best. 
count_all_coords <- count_all_coords %>%
  mutate(Log_10 = log10(count_all_coords$n)) #%>%
  #mutate(Log_2 = log(count_all_coords$n, base = 2))

```

##Visualize the Spread of the Data
A good thing would be to visualize the range of the data that we have. 

```{r}
ggplot(count_all_coords, aes(n)) +
  geom_histogram(binwidth = 1, center= 0.5)+
  coord_cartesian(xlim =c(0,100))
```

We can see that it is very left skewed. 
How does the log10-transformed data look?

```{r}
ggplot(count_all_coords, aes(Log_10)) +
  geom_histogram(binwidth = .25, center=0.125)
```

That looks much better. How about the Log_2?

```{r}
ggplot(count_all_coords, mapping = aes(Log_2)) +
  geom_histogram(binwidth = 1, center = 0.5)
```

It seems that the Log_10 data has the most normalized distribution. 

##Mapping the data on Leaflet
##Determining a color scheme. 
```{r}
#now I need to turn continuous data into a color palette. 
#function from R for leaflet package. 
log_color_palette <- colorNumeric("magma", domain = count_all_coords$Log_10)

count_all_coords <- count_all_coords %>%
  mutate(log_10_color = log_color_palette(Log_10))

count_all_coords <- count_all_coords %>%
  select(lat, lon, log_10_color)

write_csv(just_coords_and_colors_for_geoJSON, "coordsAndColorsForGeoJSON.csv")

coords_for_geoJSON <- read_csv("coordsandColorsforGeoJSON.csv")

count_all_coords <- count_all_coords %>%
  select(lat, lon)

#this creates the final txt file used for the coordinates in javascript
write.table(count_all_coords,"coords.txt",row.names = FALSE, col.names= FALSE, dec=".",sep=";",eol="\\n")

coords_for_geoJSON <- coords_for_geoJSON %>%
  select(log_10_color)

#if I make quotes false it aligns closer to the example of convert csv to array. 
write.table(coords_for_geoJSON,"colors.txt", quote = FALSE, row.names = FALSE, col.names= FALSE, dec=".",sep=";",eol="\\n")

#i am going to take the unrounded coordinates, only the latitude and longitude, no color, and plot it in leaflet to see how we should best normalize/ round the data. 
count_all_unrounded_coordinates_lat_lon <- count_all_unrounded_coordinates %>%
  select(lat, lon)


```

##Retrying with the normalized 5 digit data
```{r}
#going to try with Bmore filtered data first. 
count_all_unrounded_coordinates <- read_csv(
  "./Data/Working with the coord data/3 digs rounding process/best_4_dig_for_each_3_dig_chosen_counted_then_filtered.csv")

#Look at both Log2 and log10. Log 10 works best. 
count_all_unrounded_coordinates <- count_all_unrounded_coordinates %>%
  mutate(Log_10 = log10(count_all_unrounded_coordinates$n)) #%>%
  #mutate(Log_2 = log(count_all_coords$n, base = 2))

```

```{r}
#now I need to turn continuous data into a color palette. 
#function from R for leaflet package. 
log_color_palette <- colorNumeric("magma", domain = count_all_unrounded_coordinates$Log_10)

count_all_unrounded_coordinates <- count_all_unrounded_coordinates %>%
  mutate(log_10_color = log_color_palette(Log_10))

count_all_unrounded_coordinates <- count_all_unrounded_coordinates %>%
  select(lat, lon, log_10_color)

coords_for_geoJSON <- count_all_unrounded_coordinates %>%
  select(lat, lon)

#this creates the final txt file used for the coordinates in javascript
write.table(coords_for_geoJSON,
            "./Data/Working with the coord data/3 digs rounding process/best_4_dig_for_each_3_dig_chosen_counted_then_filtered.txt",
            row.names = FALSE, col.names= FALSE, dec=".",sep=";",eol="\\n")

count_all_coords_rgb <- count_all_unrounded_coordinates %>%
  select(log_10_color)

#if I make quotes false it aligns closer to the example of convert csv to array. 
write.table(coords_for_geoJSON,"colors.txt", quote = FALSE, row.names = FALSE, col.names= FALSE, dec=".",sep=";",eol="\\n")

```

##Change hex color list to rgb three column matrix.
```{r}
#I can use the col2rgb function. 

#each map iteration, with the t to transpose, creates a 1 * 3 matrix.
count_all_coords_rgb <- map(.x = count_all_coords$log_10_color, 
                            .f = ~ t(col2rgb(.))) 

count_all_coords_rgb <- do.call(rbind.data.frame, (count_all_coords_rgb))

#save
write_csv(count_all_coords_rgb, "All_coords_colors_RGB.csv")

#open
count_all_coords_rgb <- read_csv("C:/Users/Jesse/Programming/R/Map My Run Redone/Data/All_coords_colors_RGB.csv")



#tim salabim has his colors as decimals- he divided everything by 255. So let's try that
count_all_coords_rgb <- count_all_coords_rgb %>%
  mutate_all(funs(./255))

#the final format of the rgb needs to be as such:
#[{"r":0.26666666666666669,"g":0.00392156862745098,"b":0.32941176470588237},
#{"r":0.9921568627450981,"g":0.9058823529411765,"b":0.1450980392156863}]

#change colnames to r,g,b
colnames(count_all_coords_rgb) <- c('r', 'g', 'b')

#Making a csv of tthe colors in RGB notation for leaflet glify:
write.table(count_all_coords_rgb,"colors_csv.txt", row.names = FALSE, col.names= TRUE, dec=".",sep=";",quote= FALSE, eol="\\n")
```